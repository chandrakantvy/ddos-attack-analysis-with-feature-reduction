{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Filter Method = Improved Multi-Class.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"7FaGFAjehgwF"},"source":["# Imports\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import chi2\n","from sklearn.feature_selection import mutual_info_regression\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import confusion_matrix, classification_report, mutual_info_score\n","from sklearn.feature_selection import SelectFromModel\n","\n","from sklearn.svm import LinearSVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7iLBBO8ghgwI"},"source":["# Defining some Globals\n","RANDOM_STATE = 42\n","CATEGORICAL_FEATURE_COUNT=2\n","CORELATION_FILTER_NUM_ITERATIONS=1\n","\n","SPEARMAN_THRESHOLD=0.9\n","PEARSON_THRESHOLD=0.9\n","CHI_THRESHOLD=0.9\n","TEST_SPLIT_PROPORTION=0.8\n","\n","NUM_FOLDS=10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E50KWkWlhgwK"},"source":["# Getting the Preprocessed Dataset: Multi-Class\n","train = pd.read_csv(\"./Improved Datasets/multiclass_training_data.csv\").iloc[:, 1:]\n","test = pd.read_csv(\"./Improved Datasets/multiclass_testing_data.csv\").iloc[:, 1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybNkF_QghgwK","outputId":"81cc834b-9f03-48cd-b621-085235ac2403"},"source":["train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Flow Duration</th>\n","      <th>Total Fwd Packets</th>\n","      <th>Total Backward Packets</th>\n","      <th>Total Length of Fwd Packets</th>\n","      <th>Total Length of Bwd Packets</th>\n","      <th>Fwd Packet Length Max</th>\n","      <th>Fwd Packet Length Min</th>\n","      <th>Fwd Packet Length Mean</th>\n","      <th>Fwd Packet Length Std</th>\n","      <th>Bwd Packet Length Max</th>\n","      <th>...</th>\n","      <th>Idle Mean</th>\n","      <th>Idle Std</th>\n","      <th>Idle Max</th>\n","      <th>Idle Min</th>\n","      <th>SimillarHTTP</th>\n","      <th>Inbound</th>\n","      <th>protocol_HOPOPT</th>\n","      <th>protocol_TCP</th>\n","      <th>protocol_UDP</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>832</td>\n","      <td>0</td>\n","      <td>416</td>\n","      <td>416</td>\n","      <td>416.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>50</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1234</td>\n","      <td>0</td>\n","      <td>617</td>\n","      <td>617</td>\n","      <td>617.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>494</td>\n","      <td>0</td>\n","      <td>247</td>\n","      <td>247</td>\n","      <td>247.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 82 columns</p>\n","</div>"],"text/plain":["   Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n","0              1                  2                       0   \n","1             50                  2                       0   \n","2              1                  2                       0   \n","3              1                  2                       0   \n","4              1                  2                       0   \n","\n","   Total Length of Fwd Packets  Total Length of Bwd Packets  \\\n","0                          832                            0   \n","1                         1234                            0   \n","2                          494                            0   \n","3                           12                            0   \n","4                           12                            0   \n","\n","   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n","0                    416                    416                   416.0   \n","1                    617                    617                   617.0   \n","2                    247                    247                   247.0   \n","3                      6                      6                     6.0   \n","4                      6                      6                     6.0   \n","\n","   Fwd Packet Length Std  Bwd Packet Length Max  ...  Idle Mean  Idle Std  \\\n","0                    0.0                      0  ...        0.0       0.0   \n","1                    0.0                      0  ...        0.0       0.0   \n","2                    0.0                      0  ...        0.0       0.0   \n","3                    0.0                      0  ...        0.0       0.0   \n","4                    0.0                      0  ...        0.0       0.0   \n","\n","   Idle Max  Idle Min  SimillarHTTP  Inbound  protocol_HOPOPT  protocol_TCP  \\\n","0         0         0             0        1                0             0   \n","1         0         0             0        1                0             0   \n","2         0         0             0        1                0             0   \n","3         0         0             0        1                0             1   \n","4         0         0             0        1                0             1   \n","\n","   protocol_UDP  Label  \n","0             1      2  \n","1             1      2  \n","2             1      3  \n","3             0      4  \n","4             0      4  \n","\n","[5 rows x 82 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"TdjZF4i_hgwL"},"source":["# Removing columns with constant values\n","for column_name in train.columns:\n","    if train[column_name].nunique() == 1:\n","        train.drop(column_name, axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_6JuyURDhgwM"},"source":["# Dividing the features into continuous and categorical features\n","categorical = []\n","continuous = []\n","for column_name in train.columns:\n","    if column_name != \"Label\":\n","        if train[column_name].nunique() <= CATEGORICAL_FEATURE_COUNT:\n","            categorical.append(column_name)\n","        else:\n","            continuous.append(column_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jVOZiZOJhgwM"},"source":["# Standard Scaling all the Continuous Values\n","scaler = StandardScaler()\n","scaler.fit(train[continuous])\n","train[continuous] = scaler.transform(train[continuous])\n","test[continuous] = scaler.transform(test[continuous])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bg9A4LvhgwM"},"source":["def ContinuousVsContinuous(df, features, method, threshold=0.9):\n","    '''\n","    Helper Function to execute correlation based filter on continous features\n","    '''\n","    corrData = df.corr(method=method).abs().sample(frac=1, axis=1).loc[features, features]\n","    corrData = corrData.sample(frac=1)\n","    i = 0\n","    while len(features) > i:\n","        row = corrData[features[i]]\n","        for index in row.index:\n","            # The index correlation with itself will be 1 so it must be omitted\n","            # The indexes belonging to \"ignore\" must be omitted\n","            if row[index] > threshold and features[i] != index and index in features:\n","                features.remove(index)\n","                corrData.drop(index, inplace=True)\n","                corrData.drop(index, axis=1, inplace=True)\n","        i += 1\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"taxA8pU6hgwN"},"source":["def getChiValues(df, features1, features2):\n","    '''\n","    Helper Function to carry out chi-square test\n","    '''\n","    corrData = pd.DataFrame(dtype=np.float32, columns=features1)\n","    for index, selected_feature in enumerate(features1[:-1]):\n","        data = chi2(df[features1[index:]], df[selected_feature])[0]\n","\n","        for index2, feature in enumerate(features1[index:]):\n","            corrData.loc[selected_feature, feature] = data[index2]\n","            corrData.loc[feature, selected_feature] = data[index2]\n","\n","    corrData.iloc[-1, -1] = corrData.iloc[-2, -2]\n","    return corrData\n","\n","def CategoricalVsCategorical(df, features, threshold):\n","    '''\n","    Helper Function to execute correlation based filter on categorical features\n","    '''\n","    corrData = getChiValues(df, features, features)\n","\n","    scaler = MinMaxScaler()\n","    corrData = scaler.fit_transform(corrData)\n","\n","    corrData = pd.DataFrame(corrData, index=features, columns=features)\n","    i = 0\n","    while len(features) > i:\n","        row = corrData[features[i]]\n","        for index in row.index:\n","            # The index correlation with itself will be 1 so it must be omitted\n","            # The indexes belonging to \"ignore\" must be omitted\n","            if row[index] > threshold and features[i] != index and index in features:\n","                features.remove(index)\n","                corrData.drop(index, inplace=True)\n","                corrData.drop(index, axis=1, inplace=True)\n","        i += 1\n","\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cj29XeDHhgwN"},"source":["# Correlation-based Filtering based on Spearman's Coefficient, Pearson's Coefficient & Chi-square Test\n","subset_of_features = []\n","for i in range(CORELATION_FILTER_NUM_ITERATIONS):\n","    spearman = ContinuousVsContinuous(train, continuous.copy(), method=\"spearman\", threshold=0.9)\n","    pearson = ContinuousVsContinuous(train, continuous.copy(), method=\"pearson\", threshold=0.9)\n","    subset_of_features += spearman\n","    subset_of_features += pearson\n","    subset_of_features = list(set(subset_of_features))\n","\n","chi = CategoricalVsCategorical(train, categorical.copy(), threshold=0.9)\n","subset_of_features += [\"Label\"]\n","subset_of_features += chi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-GlLoZZXhgwN","outputId":"08b73643-7a46-438e-f767-eb5c8562d85c"},"source":["print(len(subset_of_features))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["45\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6WNXxR2ShgwO"},"source":["def relevance_scoring(features, labels, random_state, test_size):\n","    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=random_state)\n","    myModel=RandomForestClassifier(n_estimators = 100)\n","    myModel.fit(X_train, y_train)\n","    score=myModel.feature_importances_\n","    return dict((feature, (value - min(score)) / (max(score) - min(score))) for feature, value in zip(features.columns, score))\n","\n","def mutual_info_scoring(features, labels):\n","    score={}\n","    for feature in features.columns:\n","        score[feature]=mutual_info_score(features[feature], labels)\n","    score= dict((feature, (value - min(score.values())) / (max(score.values()) - min(score.values()))) for feature, value in score.items())\n","    return score\n","\n","def relevance_report(features, labels, random_state, test_size):\n","    score1 = relevance_scoring(train.drop(columns=[\"Label\"]), train[\"Label\"], random_state, test_size)\n","    score1 = pd.DataFrame(score1.values(), index=score1.keys(), columns=[\"Relevance Score\"])\n","    score_MI = mutual_info_scoring(train.drop(columns=[\"Label\"]), train[\"Label\"])\n","    score_MI = pd.DataFrame(score_MI.values(), index=score_MI.keys(), columns=[\"MI Score\"])\n","    return pd.concat([score1, score_MI], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MPXtLQzzhgwO"},"source":["# FilteredDataSet to be used from henceforth\n","backup=train.copy()\n","train=pd.DataFrame(train, columns=subset_of_features)\n","# Getting the relevance scoring criterions\n","report=relevance_report(train.drop(columns=[\"Label\"]), train[\"Label\"], RANDOM_STATE, TEST_SPLIT_PROPORTION)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wEYcse4hhgwP"},"source":["def evaluateModels(train, test, model_of_choice):\n","\n","    scores = [0] * ((len(test[\"Label\"].value_counts().index)) ** 2)\n","\n","    myModel = model_of_choice\n","    myModel.fit(train.drop(columns=[\"Label\"]), train[\"Label\"])\n","    predictions = myModel.predict(test.drop(columns=[\"Label\"]))\n","\n","    scores_macro_avg = classification_report(test[\"Label\"], predictions, output_dict=True)[\"macro avg\"]\n","    scores_benign = classification_report(test[\"Label\"], predictions, output_dict=True)[\"0\"]\n","    scores = confusion_matrix(test[\"Label\"], predictions)\n","\n","    return scores, scores_benign, scores_macro_avg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4BUzj_uhgwQ"},"source":["def generated_scores(train, test, ranked_features, model_of_choice, model_name):\n","    # Only Correlation-Based Filter\n","    selected_columns = list(ranked_features[: len(ranked_features)].index) + [\"Label\"]\n","    scores_all, s_benign_all, s_macro_avg_all = evaluateModels(pd.DataFrame(train, columns=selected_columns), pd.DataFrame(test, columns=selected_columns), model_of_choice)\n","    print(f\"{model_name}: Only Correlation Based Filter\\n\")\n","    print(scores_all)\n","    print(\"\\n\")\n","    \n","    # Best 75% Features\n","    selected_columns = list(ranked_features[: 3 * len(ranked_features) // 4].index) + [\"Label\"]\n","    scores_75, s_benign_75, s_macro_avg_75 = evaluateModels(pd.DataFrame(train, columns=selected_columns), pd.DataFrame(test, columns=selected_columns), model_of_choice)\n","    print(f\"{model_name}: Best 75% Features\\n\")\n","    print(scores_75)\n","    print(\"\\n\")\n","\n","    # Best 50% Features\n","    selected_columns = list(ranked_features[: len(ranked_features) // 2].index) + [\"Label\"]\n","    scores_50, s_benign_50, s_macro_avg_50 = evaluateModels(pd.DataFrame(train, columns=selected_columns), pd.DataFrame(test, columns=selected_columns), model_of_choice)\n","    print(f\"{model_name}: Best 50% Features\\n\")    \n","    print(scores_50)\n","    print(\"\\n\")\n","\n","    # Best 25% Features\n","    selected_columns = list(ranked_features[: len(ranked_features) // 4].index) + [\"Label\"]\n","    scores_25, s_benign_25, s_macro_avg_25 = evaluateModels(pd.DataFrame(train, columns=selected_columns), pd.DataFrame(test, columns=selected_columns), model_of_choice)\n","    print(f\"{model_name}: Best 25% Features\\n\")\n","    print(scores_25)\n","    print(\"\\n\")\n","\n","    final_scores = []\n","    final_scores.append(pd.DataFrame(s_macro_avg_all.values(), index=s_macro_avg_all.keys(), columns=[\"100%\"]))\n","    final_scores.append(pd.DataFrame(s_macro_avg_75.values(), index=s_macro_avg_75.keys(), columns=[\"75%\"]))\n","    final_scores.append(pd.DataFrame(s_macro_avg_50.values(), index=s_macro_avg_50.keys(), columns=[\"50%\"]))\n","    final_scores.append(pd.DataFrame(s_macro_avg_25.values(), index=s_macro_avg_25.keys(), columns=[\"25%\"]))\n","    final_scores = pd.concat(final_scores, axis=1)\n","    print(f\"{model_name}: Scores\\n\")\n","    print(final_scores.iloc[:-1, :])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Whxhx7AqhgwQ"},"source":["# Choosing Dependent MI Score\n","ranked_features=report[\"MI Score\"].copy()\n","ranked_features.sort_values(inplace=True, ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cgmuhifXhgwQ","outputId":"fd1c0aaf-3d15-496e-a5ba-28693a8fc2e8"},"source":["generated_scores(train, test, ranked_features, GaussianNB(), \"Naive Bayes\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Naive Bayes: Only Correlation Based Filter\n","\n","[[ 8780     0     4     0     4]\n"," [    9 18944    98     0     1]\n"," [   55  3208 54368     0     0]\n"," [   67     0 36346     1     1]\n"," [  224     0     8     0 37831]]\n","\n","\n","Naive Bayes: Best 75% Features\n","\n","[[ 8747     0     3     0    38]\n"," [   17 18929   105     0     1]\n"," [   58  2708 54865     0     0]\n"," [   76     0 36336     2     1]\n"," [   73     0     8     0 37982]]\n","\n","\n","Naive Bayes: Best 50% Features\n","\n","[[ 8690     0    52     1    45]\n"," [    5 18923   123     0     1]\n"," [   46  2495 55090     0     0]\n"," [   50     0 36364     0     1]\n"," [   63     0    18     0 37982]]\n","\n","\n","Naive Bayes: Best 25% Features\n","\n","[[ 5900     0  2550     1   337]\n"," [   32 18877   142     0     1]\n"," [   50  2044 55537     0     0]\n"," [   51     0 36364     0     0]\n"," [   46     0    18     0 37999]]\n","\n","\n","Naive Bayes: Scores\n","\n","               100%       75%       50%       25%\n","precision  0.882954  0.889928  0.692972  0.690208\n","recall     0.786147  0.787762  0.787172  0.724834\n","f1-score   0.725728  0.730144  0.731343  0.692504\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8LGmH8IFhgwR","outputId":"a8f90b95-73c8-427d-b8b0-e7f7e0b2a269"},"source":["generated_scores(train, test, ranked_features, RandomForestClassifier(n_jobs=-1), \"Random Forest\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random Forest: Only Correlation Based Filter\n","\n","[[ 8788     0     0     0     0]\n"," [    0 18955    96     1     0]\n"," [    2   822 56802     4     1]\n"," [    3     2     9 36399     2]\n"," [    2     0     0     3 38058]]\n","\n","\n","Random Forest: Best 75% Features\n","\n","[[ 8788     0     0     0     0]\n"," [    0 18953    99     0     0]\n"," [    2   826 56799     3     1]\n"," [    3     1     9 36399     3]\n"," [    2     0     0     3 38058]]\n","\n","\n","Random Forest: Best 50% Features\n","\n","[[ 8786     0     0     2     0]\n"," [    0 18951   101     0     0]\n"," [    3   821 56803     3     1]\n"," [    3     1    18 36391     2]\n"," [    4     0     2     2 38055]]\n","\n","\n","Random Forest: Best 25% Features\n","\n","[[ 8787     0     0     1     0]\n"," [    1 18955    96     0     0]\n"," [    3   822 56802     3     1]\n"," [    3     0    18 36392     2]\n"," [    5     0     2     3 38053]]\n","\n","\n","Random Forest: Scores\n","\n","               100%       75%       50%       25%\n","precision  0.991080  0.991045  0.990979  0.990953\n","recall     0.995991  0.995959  0.995847  0.995903\n","f1-score   0.993459  0.993426  0.993338  0.993352\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T6fLeRmphgwR","outputId":"73cda1a9-34a0-41f9-8ea9-9de475354f02"},"source":["generated_scores(train, test, ranked_features, LinearSVC(dual=False), \"Linear SVC\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Linear SVC: Only Correlation Based Filter\n","\n","[[ 8776     0     0     7     5]\n"," [    0 18819   230     2     1]\n"," [    2  1655 55966     8     0]\n"," [   13     8    64 36329     1]\n"," [   35     8     9     0 38011]]\n","\n","\n","Linear SVC: Best 75% Features\n","\n","[[ 8736     0     0     7    45]\n"," [    0 18819   230     2     1]\n"," [    2  1653 55968     8     0]\n"," [   13     8    67 36326     1]\n"," [   34     6     8     0 38015]]\n","\n","\n","Linear SVC: Best 50% Features\n","\n","[[ 8670     0     4    54    60]\n"," [    0 18821   228     2     1]\n"," [    2  1660 55961     8     0]\n"," [    7     0    87 36320     1]\n"," [   31     0    18     0 38014]]\n","\n","\n","Linear SVC: Best 25% Features\n","\n","[[ 5729     5    22  2573   459]\n"," [    3 18618   428     2     1]\n"," [    4  1622 55998     7     0]\n"," [   18     0   200 36196     1]\n"," [   31     0    18     0 38014]]\n","\n","\n","Linear SVC: Scores\n","\n","               100%       75%       50%       25%\n","precision  0.981349  0.981185  0.980997  0.963920\n","recall     0.990757  0.989858  0.988315  0.918699\n","f1-score   0.985772  0.985243  0.984376  0.933951\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYNpM71JhgwR"},"source":["# Choosing Dependent MI Score\n","ranked_features=report[\"Relevance Score\"].copy()\n","ranked_features.sort_values(inplace=True, ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHpAWG8ThgwR","outputId":"55ac4eb6-f340-4507-a47e-cf67e7fbd659"},"source":["generated_scores(train, test, ranked_features, GaussianNB(), \"Naive Bayes\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Naive Bayes: Only Correlation Based Filter\n","\n","[[ 8780     0     4     0     4]\n"," [    9 18944    98     0     1]\n"," [   55  3208 54368     0     0]\n"," [   67     0 36346     1     1]\n"," [  224     0     8     0 37831]]\n","\n","\n","Naive Bayes: Best 75% Features\n","\n","[[ 8747     0     3     0    38]\n"," [   17 18929   105     0     1]\n"," [   56  2708 54867     0     0]\n"," [   76     0 36336     2     1]\n"," [   70     0     8     0 37985]]\n","\n","\n","Naive Bayes: Best 50% Features\n","\n","[[ 8744     0     6     0    38]\n"," [   16 18911   124     0     1]\n"," [   58  2517 55056     0     0]\n"," [   67     0 36346     1     1]\n"," [   72     0     8     0 37983]]\n","\n","\n","Naive Bayes: Best 25% Features\n","\n","[[ 8744     0     6     0    38]\n"," [    7 18877   167     0     1]\n"," [   11  1882 55738     0     0]\n"," [   23     0 34222  2170     0]\n"," [   89     0     8     0 37966]]\n","\n","\n","Naive Bayes: Scores\n","\n","               100%       75%       50%       25%\n","precision  0.882954  0.890039  0.891829  0.902401\n","recall     0.786147  0.787785  0.788167  0.802001\n","f1-score   0.725728  0.730211  0.731298  0.760710\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f41VgeX9hgwS","outputId":"93d74e85-e132-4a37-b8f0-580d3b660483"},"source":["generated_scores(train, test, ranked_features, RandomForestClassifier(n_jobs=-1), \"Random Forest\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random Forest: Only Correlation Based Filter\n","\n","[[ 8788     0     0     0     0]\n"," [    0 18974    78     0     0]\n"," [    3   846 56778     3     1]\n"," [    3     2     9 36399     2]\n"," [    3     0     0     3 38057]]\n","\n","\n","Random Forest: Best 75% Features\n","\n","[[ 8788     0     0     0     0]\n"," [    0 18977    75     0     0]\n"," [    3   847 56778     2     1]\n"," [    3     2     9 36399     2]\n"," [    2     0     0     4 38057]]\n","\n","\n","Random Forest: Best 50% Features\n","\n","[[ 8788     0     0     0     0]\n"," [    0 18974    78     0     0]\n"," [    2   917 56708     3     1]\n"," [    3     1     9 36400     2]\n"," [    3     0     0     7 38053]]\n","\n","\n","Random Forest: Best 25% Features\n","\n","[[ 8788     0     0     0     0]\n"," [    0 18978    74     0     0]\n"," [    1   909 56715     4     2]\n"," [    4     1     8 36399     3]\n"," [    1     0     0     2 38060]]\n","\n","\n","Random Forest: Scores\n","\n","               100%       75%       50%       25%\n","precision  0.990885  0.990909  0.990211  0.990364\n","recall     0.996102  0.996133  0.995843  0.995941\n","f1-score   0.993407  0.993435  0.992926  0.993052\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3Jn01bfLhgwT","outputId":"5e81c361-260c-4cbe-8481-1634fd46c9e7"},"source":["generated_scores(train, test, ranked_features, LinearSVC(dual=False), \"Linear SVC\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Linear SVC: Only Correlation Based Filter\n","\n","[[ 8776     0     0     7     5]\n"," [    0 18819   230     2     1]\n"," [    2  1655 55966     8     0]\n"," [   12     9    66 36327     1]\n"," [   36     8     9     0 38010]]\n","\n","\n","Linear SVC: Best 75% Features\n","\n","[[ 8737     0     0     6    45]\n"," [    0 18819   230     2     1]\n"," [    2  1653 55968     8     0]\n"," [   13     8    66 36327     1]\n"," [   34     6     8     0 38015]]\n","\n","\n","Linear SVC: Best 50% Features\n","\n","[[ 8744     0     0     6    38]\n"," [    0 18876   173     2     1]\n"," [    4  1711 55910     6     0]\n"," [   13     8    61 36332     1]\n"," [   32     6     9     0 38016]]\n","\n","\n","Linear SVC: Best 25% Features\n","\n","[[ 8743     1     0     6    38]\n"," [    4 18891   154     2     1]\n"," [   11  1722 55892     6     0]\n"," [   22     0    69 36323     1]\n"," [   38     0     8     2 38015]]\n","\n","\n","Linear SVC: Scores\n","\n","               100%       75%       50%       25%\n","precision  0.981333  0.981195  0.980986  0.980461\n","recall     0.990741  0.989887  0.990476  0.990493\n","f1-score   0.985755  0.985262  0.985406  0.985146\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7Z6l3NmlhgwT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T9dJTwi_hgwT"},"source":[""],"execution_count":null,"outputs":[]}]}